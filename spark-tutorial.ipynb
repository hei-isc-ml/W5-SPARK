{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: DataFrame\n",
    "\n",
    "This is a tutorial for the PySpark [DataFrame API](https://spark.apache.org/docs/3.4.2/api/python/index.html). \n",
    "\n",
    "PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as `collect()` are explicitly called, the computation starts. \n",
    "\n",
    "This notebook shows the basic usages of the DataFrame, geared mainly for new users. You can run the latest version of these examples by yourself in ‘Live Notebook: DataFrame’ at the quickstart page. It is mostly inspired by [this tutorial](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html).\n",
    "\n",
    "Refer to https://spark.apache.org/docs/latest/api/python/reference/index.html for the API reference (bookmark it!).\n",
    "\n",
    "PySpark applications start with initializing SparkSession which is the entry point of PySpark as below. In case of running it in PySpark shell via pyspark executable, the shell automatically creates the session in the variable spark for users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import pyspark\n",
    "import sys\n",
    "import altair as alt\n",
    "\n",
    "# Set rendering options\n",
    "alt.renderers.enable(\"html\")\n",
    "\n",
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Pyarrow version: \", pyarrow.__version__)\n",
    "print(\"Pyspark version: \", pyspark.__version__)\n",
    "print(\"Python version: \", sys.version)\n",
    "\n",
    "\n",
    "# You may also use Spark Connect to connect to a remote cluster, but this may be limiting for this specific tutorial\n",
    "# spark = SparkSession.builder.remote(\"sc://vlenpmod302spk1.hevs.ch:15002\").getOrCreate()\n",
    "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set up [black](https://github.com/psf/black), which is a highly encouraged best-practice for all your Python projects. That way, you never have to worry and debate about code formatting anymore. By using it, you agree to cede control over minutiae of hand-formatting. In return, Black gives you speed, determinism, and freedom from `pycodestyle` nagging about formatting. You will save time and mental energy for more important matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "# Ensure that the black formatter is loaded\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PySpark DataFrame can be created via `pyspark.sql.SparkSession.createDataFrame` typically by passing a list of lists, tuples, dictionaries and `pyspark.sql.Row`s, a pandas DataFrame and an RDD consisting of such a list. `pyspark.sql.SparkSession.createDataFrame` takes the schema argument to specify the schema of the DataFrame. When it is omitted, PySpark infers the corresponding schema by taking a sample from the data.\n",
    "\n",
    "Firstly, you can create a PySpark DataFrame from a list of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        Row(a=1, b=2.0, c=\"string1\", d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "        Row(a=2, b=3.0, c=\"string2\", d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "        Row(a=4, b=5.0, c=\"string3\", d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0)),\n",
    "    ]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PySpark DataFrame with an explicit schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame(\n",
    "    [\n",
    "        (1, 2.0, \"string1\", date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "        (2, 3.0, \"string2\", date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "        (3, 4.0, \"string3\", date(2000, 3, 1), datetime(2000, 1, 3, 12, 0)),\n",
    "    ],\n",
    "    schema=\"a long, b double, c string, d date, e timestamp\",\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you pass data that are not compatible with the schema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "# Try here different types of data and describe what happens.\n",
    "\n",
    "# The following leads to a `PySparkTypeError` because \"4.0\" cannot be cast to a double.\n",
    "# Note however that passing a float to a string column will not raise an error, but the value will be cast to a string.\n",
    "try:\n",
    "    df2 = spark.createDataFrame(\n",
    "        [\n",
    "            (1, 2.0, \"string1\", date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "            (2, 3.0, \"string2\", date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "            (3, \"4.0\", \"string3\", date(2000, 3, 1), datetime(2000, 1, 3, 12, 0)),\n",
    "        ],\n",
    "        schema=\"a long, b double, c string, d date, e timestamp\",\n",
    "    )\n",
    "except pyspark.errors.PySparkTypeError as e:\n",
    "    print(\"PySparkTypeError: \", e)\n",
    "except pa.lib.ArrowInvalid as e:\n",
    "    print(\"ArrowInvalid: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PySpark DataFrame from a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = pd.DataFrame(\n",
    "    {\n",
    "        \"a\": [1, 2, 3],\n",
    "        \"b\": [2.0, 3.0, 4.0],\n",
    "        \"c\": [\"string1\", \"string2\", \"string3\"],\n",
    "        \"d\": [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "        \"e\": [\n",
    "            datetime(2000, 1, 1, 12, 0),\n",
    "            datetime(2000, 1, 2, 12, 0),\n",
    "            datetime(2000, 1, 3, 12, 0),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "df3 = spark.createDataFrame(df_pd)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrames created above all have the same results and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All DataFrames above result same.\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df2.printSchema()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Data\n",
    "\n",
    "The top rows of a DataFrame can be displayed using `DataFrame.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can enable `spark.sql.repl.eagerEval.enabled` configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter. The number of rows to show can be controlled via `spark.sql.repl.eagerEval.maxNumRows` configuration.\n",
    "\n",
    "**Note**: we set this configuration for the purpose of this tutorial, but it is a good practice to explicitly use `.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.maxNumRows\", 5)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows can also be shown vertically. This is useful when rows are too long to show horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the DataFrame’s schema and column names as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a parallelism between the Python API and SQL. Given the above schema, if you wanted to only select columns `a`, `b`, and `c`, you would write the following SQL:\n",
    "\n",
    "```sql\n",
    "SELECT a, b, c FROM df;\n",
    "```\n",
    "\n",
    "Equivalently, you can write the following Python code using the Dataframe API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"a\", \"b\", \"c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"myTable\")\n",
    "spark.sql(\"SELECT a, b, c FROM myTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do all the good stuff using SQL (and we will see later how to write an equivalent query using the Dataframe API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT sum(a) as sumA, avg(b) as avgB FROM myTable GROUP BY c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have a large dataframe and you want to understand what it contains, the `describe()` function comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"a\", \"b\", \"c\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame.collect()` collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `collect()` will fetch **all** data from **all** workers, thus leading to a large memory load. In order to avoid throwing an out-of-memory exception, use `DataFrame.take()` or `DataFrame.tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that `toPandas` also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Access Data\n",
    "\n",
    "PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a `Column` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "type(df.c) == type(F.upper(df.c)) == type(df.c.isNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `Column`s can be used to select the columns from a DataFrame. For example, `DataFrame.select()` takes the `Column` instances that returns another DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.c).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also refer to columns by their name, or using `pyspark.sql.functions.col` when there is ambiguity (e.g., when complex expressions must be built):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three ways to select columns\n",
    "df.select(F.col(\"a\"), df.b, \"c\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `withColumn` to add a new column to a dataframe, e.g., computed from another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"upper_c\", F.upper(df.c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select a subset of rows, use `DataFrame.filter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(F.col(\"a\") == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Data\n",
    "\n",
    "Up to now, we have seen very basic manipulations. We are going to go a bit deeper into data manipulations. We are going to create a slightly more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "colors = [\"red\", \"blue\", \"black\", \"green\"]\n",
    "fruits = [\"banana\", \"grape\", \"carrot\", \"pear\", \"apple\"]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        [random.choice(colors), random.choice(fruits), idx, random.randint(0, 1000)]\n",
    "        for idx in range(100)\n",
    "    ],\n",
    "    schema=\"color string, fruit string, id long, calories int\",\n",
    ")\n",
    "\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by looking at transforming columns.\n",
    "\n",
    "Say you want to create a new column that contains the concatenation of the color and the fruit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(F.concat(F.col(\"color\"), F.lit(\" \"), F.col(\"fruit\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is what we want, but the column name isn't really nice. We actually want to `alias` the column, as you would do in SQL with `SELECT CONCAT(color, fruit) AS name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(F.concat(F.col(\"color\"), F.lit(\" \"), F.col(\"fruit\")).alias(\"name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, this looks better, but we lost all other columns. Let's fix this and overwrite the working dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"*\", F.concat(F.col(\"color\"), F.lit(\" \"), F.col(\"fruit\")).alias(\"name\"))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a myriad of different [functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html) that one can use. Just as an example, here are a few things you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    # How many characters are in the `name` column\n",
    "    F.length(\"name\").alias(\"name_length\"),\n",
    "    # Convert the `color` column to uppercase\n",
    "    F.upper(\"color\").alias(\"COLOR\"),\n",
    "    # Levenshtein distance between the `color` and `fruit`\n",
    "    F.levenshtein(\"color\", \"fruit\").alias(\"color_to_fruit_dist\"),\n",
    "    # Logarithm of the `calories` column\n",
    "    F.log2(\"calories\").alias(\"log2_calories\"),\n",
    "    # Product of the number of characters in the `name` column and the `calories` column\n",
    "    (F.length(\"name\") * F.col(\"calories\")).alias(\"complex_calc\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the above calculations are plain useless, but the point is that you can perform complex calculations on very large datasets, as all these operations are going to be automatically parallelized across all the workers.\n",
    "\n",
    "### Applying custom functions\n",
    "\n",
    "> **NOTE**: this section requires a local cluster as Pandas is not installed on the workers. \n",
    "\n",
    "There will be situations where there is no available built-in function to do what you want. In this case, you can use what are called User-Defined Functions (UDFs), which can be written in plain Python or in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "@udf(\"long\")\n",
    "def plus_one(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Add one to the series\n",
    "    return series + 1\n",
    "\n",
    "\n",
    "df_udfs = df.select(\n",
    "    # Use a plain UDF to add one to the `calories` column\n",
    "    plus_one(F.col(\"calories\").cast(\"long\")).alias(\"calories_plus_one_udf\"),\n",
    "    # Use a Pandas UDF to add one to the `calories` column\n",
    "    pandas_plus_one(F.col(\"calories\").cast(\"long\")).alias(\n",
    "        \"calories_plus_one_pandas_udf\"\n",
    "    ),\n",
    "    # Which is of course equivalent to this:\n",
    "    (F.col(\"calories\") + 1).alias(\"calories_plus_one_sql\"),\n",
    ")\n",
    "\n",
    "df_udfs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test whether this is equivalent by filtering on rows that differ in the resulting dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows_diff = df_udfs.filter(\n",
    "    (F.col(\"calories_plus_one_udf\") != F.col(\"calories_plus_one_sql\"))\n",
    "    | (F.col(\"calories_plus_one_pandas_udf\") != F.col(\"calories_plus_one_sql\"))\n",
    ").count()\n",
    "\n",
    "print(\n",
    "    f\"As expected, there are {num_rows_diff} rows where the UDF and SQL results differ.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was pretty boring. Imagine instead that you want to have some dedicated logic to convert calories to string descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert calories to a description\n",
    "@udf(\"string\")\n",
    "def calories_to_descr_udf(calories: int) -> str:\n",
    "    if calories < 100:\n",
    "        return \"low\"\n",
    "    elif calories < 500:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def calories_to_descr_pandas_udf(calories: pd.Series) -> pd.Series:\n",
    "    # Preallocate a Series of the same length as the input and of type string\n",
    "    descr = pd.Series(data=\"high\", index=calories.index, dtype=str)\n",
    "\n",
    "    # Assign the description based on the calorie content\n",
    "    descr[calories < 100] = \"low\"\n",
    "    descr[(calories >= 100) & (calories < 500)] = \"medium\"\n",
    "\n",
    "    return descr\n",
    "\n",
    "\n",
    "df.select(\n",
    "    calories_to_descr_udf(\"calories\").alias(\"calories_descr_udf\"),\n",
    "    calories_to_descr_pandas_udf(\"calories\").alias(\"calories_descr_pandas_udf\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, it's generally recommended to use Spark's built-in functions when possible, and only resort to UDFs when necessary for custom logic that can't be achieved otherwise. In reality, the above can be achieved using Spark built-in functions as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    F.when(F.col(\"calories\") < 100, \"low\")\n",
    "    .when((F.col(\"calories\") >= 100) & (F.col(\"calories\") < 500), \"medium\")\n",
    "    .otherwise(\"high\")\n",
    "    .alias(\"calories_descr\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time all solutions on a much bigger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "# Build a dataset with 5000 rows\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        [random.choice(colors), random.choice(fruits), idx, random.randint(0, 1000)]\n",
    "        for idx in range(50_000)\n",
    "    ],\n",
    "    schema=\"color string, fruit string, id long, calories int\",\n",
    ")\n",
    "\n",
    "# Time the plain UDF solution\n",
    "num_secs = timeit.timeit(\n",
    "    stmt=\"df.select(calories_to_descr_udf('calories').alias('calories_descr')).collect()\",\n",
    "    globals=globals(),\n",
    "    number=10,\n",
    ")\n",
    "print(f\"UDF solution took {num_secs:.2f} seconds\")\n",
    "\n",
    "# Time the Pandas UDF solution\n",
    "num_secs = timeit.timeit(\n",
    "    stmt=\"df.select(calories_to_descr_pandas_udf('calories').alias('calories_descr')).collect()\",\n",
    "    globals=globals(),\n",
    "    number=10,\n",
    ")\n",
    "print(f\"Pandas UDF solution took {num_secs:.2f} seconds\")\n",
    "\n",
    "# Now let's see how long the SQL solution takes\n",
    "stmt = \"\"\"\n",
    "df.select(\n",
    "    F.when(F.col(\"calories\") < 100, \"low\")\n",
    "    .when((F.col(\"calories\") >= 100) & (F.col(\"calories\") < 500), \"medium\")\n",
    "    .otherwise(\"high\")\n",
    "    .alias(\"calories_descr\")\n",
    ").collect()\n",
    "\"\"\".strip()\n",
    "\n",
    "num_secs = timeit.timeit(stmt=stmt, globals=globals(), number=10)\n",
    "print(f\"SQL solution took {num_secs:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out what is going on here? Read [this article](https://bryancutler.github.io/vectorizedUDFs/) to understand a bit more about what's happening under the hood.\n",
    "\n",
    "Again, there will be situations where you will want to use UDFs, but that should be the last recourse. You should also test your solutions before making a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Joining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame.\n",
    "\n",
    "Grouping and then applying the `avg()` function to the resulting groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"color\").avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply a Python native function against each group by using pandas API.\n",
    "\n",
    "For instance, here we shift the `calories` value by the mean of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mean(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return data.assign(calories=data.calories - data.calories.mean())\n",
    "\n",
    "\n",
    "df.groupby(\"color\").applyInPandas(remove_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform joins. For instance, say you have a dataframe `orders` that contains orders from different customers with the amount, and a dataset `customers` that contains the name and location of each customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.createDataFrame(\n",
    "    [(1, \"A\", 100), (2, \"B\", 200), (3, \"A\", 150), (4, \"C\", 300), (5, \"B\", 250)],\n",
    "    [\"order_id\", \"customer_id\", \"amount\"],\n",
    ")\n",
    "\n",
    "customers = spark.createDataFrame(\n",
    "    [\n",
    "        (\"A\", \"Alice\", \"New York\"),\n",
    "        (\"B\", \"Bob\", \"Los Angeles\"),\n",
    "        (\"C\", \"Charlie\", \"Chicago\"),\n",
    "        (\"D\", \"David\", \"Houston\"),\n",
    "    ],\n",
    "    [\"customer_id\", \"name\", \"city\"],\n",
    ")\n",
    "\n",
    "orders.show()\n",
    "customers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want the total amount spent in each city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join orders with customers, using the `customer_id` column, only keep the `amount` and `city` columns\n",
    "orders_with_city = orders.select(\"customer_id\", \"amount\").join(\n",
    "    customers.select(\"customer_id\", \"city\"), on=\"customer_id\"\n",
    ")\n",
    "\n",
    "# Now group by `city` and calculate the sum of the `amount` column\n",
    "orders_with_city.groupBy(\"city\").sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataset that has the following schema:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- customer_id: string (nullable = true)\n",
    " |-- num_orders: long (nullable = false)\n",
    " |-- orders: array (nullable = false)\n",
    " |    |-- element: long (containsNull = false)\n",
    " |-- total_amount: long (nullable = false)\n",
    " ```\n",
    "\n",
    "where `num_orders` is the number of orders for each customer, `orders` is a list of `order_id`s, and `total_amount` is the total amount spent by the customer (it should be zero if no order has been made by the customer).\n",
    "\n",
    "Hint: you will need the aggregation function [`collect_list`](https://spark.apache.org/docs/3.5.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.collect_list.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data In and Out\n",
    "\n",
    "CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster.\n",
    "\n",
    "There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc. See also the latest [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html) in Apache Spark documentation.\n",
    "\n",
    "The following command will allow you to read a dataframe that is actually the result that you should have obtained before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"customers_summary.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the results are the same. \n",
    "\n",
    "Now, let's write it to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"customers_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops! This doesn't work. Can you figure out why?\n",
    "\n",
    "Anyway, this is why CSV should be generally avoided to store data. It isn't compressed, and it is not _columnar_. Over the years, a plethora of open-source data formats have been designed to support the needs of various applications. These formats can be _row_ or _column_ oriented and can support various forms of serialization and compression. \n",
    "\n",
    "The columnar data formats such as Parquet are a popular choice for fast analytics workloads. As opposed to row-oriented storage, columnar storage can significantly reduce the amount of data fetched from disk by allowing access to only the columns that are relevant for the particular query or workload. Moreover, columnar storage combined with efficient encoding and compression techniques can drastically reduce the storage requirements without sacrificing query performance.\n",
    "\n",
    "For instance, with the following, Spark will perform a predicate pushdown and only read on disk the columns `customer_id` and `total_amount`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"customers_summary.parquet\").select(\n",
    "    \"customer_id\", \"total_amount\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With CSV, it would have to load all the data in memory, and then drop what is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL\n",
    "\n",
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. If you remember, we had registered a table before, let's see if it is still there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW VIEWS;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it should be there under `mytable`. Let's see its schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE TABLE mytable;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT sum(a) as sumA, avg(b) as avgB FROM myTable GROUP BY c\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a reference to the table in Python world and then use the Dataframe API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"myTable\")\n",
    "df.groupBy(\"c\").agg(F.sum(\"a\").alias(\"sumA\"), F.avg(\"b\").alias(\"avgB\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a difference here: the Dataframe API automatically selects the column(s) used in the group-by, whereas SQL would require to an explicit selection. To get an exact match, we need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT c, sum(a) as sumA, avg(b) as avgB FROM myTable GROUP BY c\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with window functions\n",
    "\n",
    "Window functions are more advanced constructs, but they are critical when dealing with time-series data, which is going to be the case in the rest of this module. Window functions are powerful tools in PySpark that allow you to perform calculations across a set of rows that are related to the current row. They are particularly useful for tasks like running totals, rankings, and moving averages.\n",
    "\n",
    "Assume for instance that we have a dataframe of employees, their role, and salary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"Sales\", 5000),\n",
    "    (\"Bob\", \"Sales\", 4000),\n",
    "    (\"Charlie\", \"Marketing\", 4500),\n",
    "    (\"David\", \"Sales\", 6000),\n",
    "    (\"Eve\", \"Marketing\", 5500),\n",
    "    (\"Lisa\", \"Marketing\", 5500),\n",
    "    (\"Meghan\", \"Sales\", 5000),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Functions\n",
    "\n",
    "Let's start with a simple window function to assign row numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the window: partition by department, order by salary\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "# Compute the row number in the window for each row\n",
    "df_with_row_number = df.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "\n",
    "df_with_row_number.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we do here? We computed the number of each row the window it belongs:\n",
    "\n",
    "```\n",
    "+-------+----------+------+----------+\n",
    "|   name|department|salary|row_number|\n",
    "+-------+----------+------+----------+\n",
    "|Charlie| Marketing|  4500|         1| <-- this row is the 1st in the 'Marketing' window, ordered by (ascending) salaries\n",
    "|    Eve| Marketing|  5500|         2| <-- this row is the 2nd in the 'Marketing' window, order by (ascending) salaries\n",
    "|   Lisa| Marketing|  5500|         3|\n",
    "|    Bob|     Sales|  4000|         1|\n",
    "                  ...\n",
    "+-------+----------+------+----------+\n",
    "```\n",
    "\n",
    "Now, this is extremely useful if you are asked to report the top salaries by function in an organization. Write the query that would answer this question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a query that reports the top employee by salary in each department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are actually several ranking functions:\n",
    "- `row_number` just enumerates the row. If two rows have the same value used by the `orderBy`, they will be assigned different values in **a non-deterministic fashion**.\n",
    "- `rank` calculates the rank of the row in the window in a similar way you would rank runners in a race. It means that if two rows have the same rank `n`, the next row will have the rank `n+2`.\n",
    "- `dense_rank` is the same as `rank`, but it doesn't leave any gap: if two rows have the same rank `n`, the next row will have the rank `n+1`.\n",
    "\n",
    "You can check that this is correct here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "    .withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "    .withColumn(\"dense_rank\", F.dense_rank().over(windowSpec))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Functions\n",
    "\n",
    "Similarly to a group-by, one can use `avg`, `sum`, etc. as window functions. For instance, if you want to compare the salary of a given person with the average salary in their department:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"department\")\n",
    "\n",
    "df_with_avg = df.withColumn(\n",
    "    \"avg_salary_in_department\", F.avg(\"salary\").over(windowSpec)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a query to identify people who are below one standard deviation of the average salary of their department:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your query here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your query should return `Charlie` and `Bob` as being the two underpaid employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead and Lag Functions\n",
    "\n",
    "Lead and lag functions allow you to access data from other rows relative to the current row. This requires an `orderBy` clause in the window definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df_with_lead_lag = df.withColumn(\n",
    "    \"next_salary\", F.lead(\"salary\", 1).over(windowSpec)\n",
    ").withColumn(\"prev_salary\", F.lag(\"salary\", 1).over(windowSpec))\n",
    "\n",
    "df_with_lead_lag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `lead` and `lag` functions are particularly useful when dealing with time-series, as one can compute differences between two successive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time-indexed dataframe with a value and category column\n",
    "data = [\n",
    "    (datetime.fromisoformat(\"2020-01-01\"), 100, \"A\"),\n",
    "    (datetime.fromisoformat(\"2020-01-02\"), 120, \"A\"),\n",
    "    (datetime.fromisoformat(\"2020-01-03\"), 130, \"A\"),\n",
    "    (datetime.fromisoformat(\"2020-01-01\"), 200, \"B\"),\n",
    "    (datetime.fromisoformat(\"2020-01-02\"), 210, \"B\"),\n",
    "    (datetime.fromisoformat(\"2020-01-03\"), 220, \"B\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"date\", \"measurement\", \"sensor\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query that computes the average daily variation of each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your query here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe should look like this:\n",
    "```\n",
    "+------+-------------+\n",
    "|sensor|avg_variation|\n",
    "+------+-------------+\n",
    "|     A|         15.0|\n",
    "|     B|         10.0|\n",
    "+------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing data\n",
    "\n",
    "Unfortunately, Spark does not offer any built-in visualization. One must first materialize data as Pandas dataframe and then plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().plot.line(x=\"date\", y=\"measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a big problem with the above plot. What is it? Can you try to fix it and have a nicer plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain what's wrong with this?\n",
    "\n",
    "# Any idea how to fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting data\n",
    "\n",
    "One problem here is that we have a dataframe in a so-called 'long' format: we have one row per sensor, and a column `sensor` that denotes which row belongs to which sensor. Plotting these data isn't very easy, as the points overlap.\n",
    "\n",
    "Instead, we should have a dataframe in so-called 'wide' format, where one row corresponds to one measurement, with multiple columns per sensor. To achieve this, we need to perform a `pivot`, which consists in four steps:\n",
    "\n",
    "1. Choose a pivot column: This column's unique values will become the new column headers. In our case, this is the `sensor` column.\n",
    "2. Select a value column: The data from this column will fill the cells in the new table. In our case, this is the `measurement` column.\n",
    "3. Identify a grouping column: This column (or columns) will define the rows of the new table. In our case, this is the `date` column.\n",
    "4. Reshape the data: The pivoting operation then reorganizes your data based on these choices.\n",
    "\n",
    "In PySpark, pivoting is expressed as an operation on a `GroupedData` expression, where the grouping key is the `date`. We then `pivot` on the `sensor` column, and aggregate `measurements` that correspond to a unique combination of `date` and `sensor`. Note that we use `F.first` here as we expect only one measurement per date and sensor, but we could in principle average, sum, or take the standard deviation across multiple measurements.\n",
    "\n",
    "Before plotting the data, we must re-order by `date`, as the pivoting processing will shuffle the order of the rows. Finally, we can simply plot the resulting Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = (\n",
    "    df.groupBy(\"date\")\n",
    "    .pivot(\"sensor\")\n",
    "    .agg(F.first(\"measurement\"))\n",
    "    .orderBy(\"date\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "df_pd.plot.line(x=\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot used [Matplotlib](https://matplotlib.org/stable/) as backend, and it is rendered as a static image. Other backends are supported by Pandas and can be helpful for interactive visualization, such as [Plotly](https://plotly.com/python/pandas-backend/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# Different backends might require different arguments\n",
    "df_pd.plot.line(x=\"date\", y=[\"A\", \"B\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick word about Pandas \n",
    "\n",
    "This tutorial has been using very small datasets, for which Spark isn't really fit-for-purpose. Very often, you might be tempted to simply do `toPandas()` and then work with the Pandas API. This is a **very bad idea** for at least two reasons:\n",
    "\n",
    "1. The code that you produce will **NOT** scale to larger datasets. If you picked Spark in the first place, it is probably because you intend your code to scale up to hundreds of GBs, maybe dozens of TBs. Remember than whenever you do a `toPandas()`, you essentially bring back the result of your query in a single node (the master node), which can cause Out-Of-Memory (OOM) errors and will slow down massively your computation.\n",
    "2. If you know that the result of your query will fit in memory, you might be tempted to work with Pandas anyway. Even then, I would strongly suggest you don't. Instead, use [Polars](https://pola.rs/), which is a much faster, easy-to-use, and robust library than Pandas. It is written in Rust (which makes it **a lot** faster than Pandas) and its [API is much closer to PySpark](https://docs.pola.rs/user-guide/migration/spark/) (and SQL) than the [rather poorly designed Pandas API](https://docs.pola.rs/user-guide/migration/pandas/).\n",
    "\n",
    "In particular, Polars support lazy queries, similar to Spark, which allows it to perform query optimizations (in the same as the Catalyst engine does), work with larger-than-memory datasets using streaming, or catch schema errors before processing data. Generally speaking, its API is cleaner and better constructed than Pandas'. \n",
    "\n",
    "So, instead of a plain `toPandas()`, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "print(\"Polars version: \", pl.__version__)\n",
    "\n",
    "# Convert a Spark dataframe to Polars\n",
    "df_pl = pl.from_pandas(df.toPandas())\n",
    "df_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data using Polars and Altair, which doesn't require a pivot\n",
    "df_pl.plot.line(x=\"date\", y=\"measurement\", color=\"sensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars can be a very effective solution for smaller dataset (up to a few GBs). If you are on a beefy server with lots of RAM and CPU cores, it can crunch datasets up to a few hundreds GBs.\n",
    "\n",
    "### GPU acceleration\n",
    "\n",
    "More recently, Polars and NVIDIA engineers started to collaborate on [RAPIDS](https://rapids.ai/), which aims at bringing [GPU acceleration to Polars DataFrames](https://pola.rs/posts/polars-on-gpu/). Using this strategy, Polars is able to fully utilize both the CPU and GPU (if available) significantly speeding up certain workloads. Note that a similar integration is being designed for [Apache Spark](https://nvidia.github.io/spark-rapids/).\n",
    "\n",
    "For a list of curated resources about Polars, check out [Awesome Polars](https://github.com/ddotta/awesome-polars).\n",
    "\n",
    "That's it, thanks a lot for following this tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve me!\n",
    "\n",
    "Now, the goal is that you extend or improve this tutorial so that the students have a better experience next year! Here are five things you can choose and focus on:\n",
    "\n",
    "- **Optimizing Data Processing with Catalyst Optimizer and Tungsten**: Extend the tutorial to illustrate the optimization performed by the query optimizer (Catalyst) and the execution engine (Tungsten), in particular looking at the logical and physical plans.\n",
    "- **Performance Tuning and Partitioning Strategies**: Explore and discuss performance tuning and partitioning strategies, especially when data are skewed. Generate scenarios wiht data skews and explore what happens in terms of performance with multiple workers.\n",
    "- **Structured Streaming with Spark**: How to process real-time data by intesting a streaming dataset and applying real-time transformations, aggregations, or windowing operations. \n",
    "- **Window functions**: Improve the section on window functions with more didactic or more complex examples. \n",
    "- **Spark vs competitors**: Explore and discuss the differences between Apache Spark, [Polars](https://github.com/pola-rs/polars), and [DuckDB](https://duckdb.org/docs/installation/?version=stable&environment=cli&platform=linux&download_method=direct&architecture=x86_64). Compare the performance between these on the same queries over a benchmark dataset.\n",
    "\n",
    "You will then get the opportunity to review the tutorial of a fellow student and learn something new in the process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isc-mod302",
   "language": "python",
   "name": "isc-mod302"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
